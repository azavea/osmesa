{"paragraphs":[{"text":"import org.apache.hadoop.hbase._\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.hbase.client._\nimport org.apache.hadoop.hbase.util._\nimport org.apache.hadoop.fs.Path\n\nimport org.apache.spark._\nimport org.apache.spark.sql._\n\nimport scala.collection.mutable.WrappedArray\nimport java.nio._\n\n","user":"anonymous","dateUpdated":"2017-11-08T22:24:50+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.hadoop.hbase._\n\nimport org.apache.hadoop.conf.Configuration\n\nimport org.apache.hadoop.hbase.client._\n\nimport org.apache.hadoop.hbase.util._\n\nimport org.apache.hadoop.fs.Path\n\nimport org.apache.spark._\n\nimport org.apache.spark.sql._\n\nimport scala.collection.mutable.WrappedArray\n\nimport java.nio._\n"}]},"apps":[],"jobName":"paragraph_1510161386018_-633700675","id":"20171108-171626_1065867155","dateCreated":"2017-11-08T17:16:26+0000","dateStarted":"2017-11-08T22:24:50+0000","dateFinished":"2017-11-08T22:24:57+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3054"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510170376493_-663765587","id":"20171108-194616_1161487447","dateCreated":"2017-11-08T19:46:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3777","text":"       /* Settings compatible for both local and EMR execution */\n      val sc = new SparkConf()\n        .setIfMissing(\"spark.master\", \"local[*]\")\n        .setAppName(\"vp-orc-io\")\n\n      implicit val ss: SparkSession = SparkSession.builder\n        .config(sc)\n        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n        .config(\"spark.kryo.registrator\", \"geotrellis.spark.io.kryo.KryoRegistrator\")\n        .enableHiveSupport\n        .getOrCreate\n","dateUpdated":"2017-11-08T20:08:57+0000","dateFinished":"2017-11-08T20:08:58+0000","dateStarted":"2017-11-08T20:08:57+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nsc: org.apache.spark.SparkConf = org.apache.spark.SparkConf@4f788d06\n\nss: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2685d57c\n"}]}},{"text":"val hbConf = org.apache.hadoop.hbase.HBaseConfiguration.create()\nhbConf.addResource(new Path(\"/etc/hbase/conf/hbase-site.xml\"))\nhbConf.addResource(new Path(\"/etc/hadoop/conf/hadoop-site.xml\"))\nval connection = ConnectionFactory.createConnection(hbConf)\nval hbAdmin = connection.getAdmin()","user":"anonymous","dateUpdated":"2017-11-08T19:58:01+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nhbConf: org.apache.hadoop.conf.Configuration = Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml, hbase-default.xml, hbase-site.xml\n\nconnection: org.apache.hadoop.hbase.client.Connection = hconnection-0x48644a4\n\nhbAdmin: org.apache.hadoop.hbase.client.Admin = org.apache.hadoop.hbase.client.HBaseAdmin@3ba9b2eb\n"}]},"apps":[],"jobName":"paragraph_1510161449967_-1858448369","id":"20171108-171729_1789589926","dateCreated":"2017-11-08T17:17:29+0000","dateStarted":"2017-11-08T19:58:01+0000","dateFinished":"2017-11-08T19:58:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3055"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510177798956_-1837483481","id":"20171108-214958_787748727","dateCreated":"2017-11-08T21:49:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4678","text":"val iom = ss.read.orc(\"s3://geotrellis-test/nathan/turkmenistan.orc\")","dateUpdated":"2017-11-08T21:50:02+0000","dateFinished":"2017-11-08T21:50:03+0000","dateStarted":"2017-11-08T21:50:02+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\niom: org.apache.spark.sql.DataFrame = [id: bigint, type: string ... 11 more fields]\n"}]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510178039389_104884376","id":"20171108-215359_943939440","dateCreated":"2017-11-08T21:53:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4815","text":"val NODE_TABLE = Bytes.toBytes(\"osm_nodes\")\nval NODE_META_CF = Bytes.toBytes(\"node_meta\")\nval NODE_TAG_CF = Bytes.toBytes(\"node_tags\")\n\nval WAY_TABLE = Bytes.toBytes(\"osm_ways\")\nval WAY_META_CF = Bytes.toBytes(\"way_meta\")\nval WAY_TAG_CF = Bytes.toBytes(\"way_tags\")\n\nval COLUMN_NODES = Bytes.toBytes(\"nodes\")\nval COLUMN_FID = Bytes.toBytes(\"id\")\nval COLUMN_LAT = Bytes.toBytes(\"lat\")\nval COLUMN_LON = Bytes.toBytes(\"lon\")\nval COLUMN_NODES = Bytes.toBytes(\"nodes\")\nval COLUMN_USER = Bytes.toBytes(\"user\")\nval COLUMN_UID = Bytes.toBytes(\"uid\")\nval COLUMN_CHANGESET = Bytes.toBytes(\"changeset\")\nval COLUMN_VERSION = Bytes.toBytes(\"version\")\nval COLUMN_TIMESTAMP = Bytes.toBytes(\"timestamp\")\nval COLUMN_VISIBLE = Bytes.toBytes(\"visible\")","dateUpdated":"2017-11-08T21:57:08+0000","dateFinished":"2017-11-08T21:57:18+0000","dateStarted":"2017-11-08T21:57:08+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nNODE_TABLE: Array[Byte] = Array(111, 115, 109, 95, 110, 111, 100, 101, 115)\n\nNODE_META_CF: Array[Byte] = Array(110, 111, 100, 101, 95, 109, 101, 116, 97)\n\nNODE_TAG_CF: Array[Byte] = Array(110, 111, 100, 101, 95, 116, 97, 103, 115)\n\nWAY_TABLE: Array[Byte] = Array(111, 115, 109, 95, 119, 97, 121, 115)\n\nWAY_META_CF: Array[Byte] = Array(119, 97, 121, 95, 109, 101, 116, 97)\n\nWAY_TAG_CF: Array[Byte] = Array(119, 97, 121, 95, 116, 97, 103, 115)\n\nCOLUMN_NODES: Array[Byte] = Array(110, 111, 100, 101, 115)\n\nCOLUMN_FID: Array[Byte] = Array(105, 100)\n\nCOLUMN_LAT: Array[Byte] = Array(108, 97, 116)\n\nCOLUMN_LON: Array[Byte] = Array(108, 111, 110)\n\nCOLUMN_NODES: Array[Byte] = Array(110, 111, 100, 101, 115)\n\nCOLUMN_USER: Array[Byte] = Array(117, 115, 101, 114)\n\nCOLUMN_UID: Array[Byte] = Array(117, 105, 100)\n\nCOLUMN_CHANGESET: Array[Byte] = Array(99, 104, 97, 110, 103, 101, 115, 101, 116)\n\nCOLUMN_VERSION: Array[Byte] = Array(118, 101, 114, 115, 105, 111, 110)\n\nCOLUMN_TIMESTAMP: Array[Byte] = Array(116, 105, 109, 101, 115, 116, 97, 109, 112)\n\nCOLUMN_VISIBLE: Array[Byte] = Array(118, 105, 115, 105, 98, 108, 101)\n"}]}},{"user":"anonymous","dateUpdated":"2017-11-08T21:59:20+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510161456310_244204370","id":"20171108-171736_338575439","dateCreated":"2017-11-08T17:17:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3056","text":"val nodeTdescriptor = new HTableDescriptor(TableName.valueOf(NODE_TABLE))\nnodeTdescriptor.addFamily(new HColumnDescriptor(NODE_META_CF))\nnodeTdescriptor.addFamily(new HColumnDescriptor(NODE_TAG_CF))\nif (! hbAdmin.tableExists(TableName.valueOf(NODE_TABLE))) hbAdmin.createTable(nodeTdescriptor)","dateFinished":"2017-11-08T19:59:32+0000","dateStarted":"2017-11-08T19:59:29+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nwarning: Class org.apache.hadoop.hbase.classification.InterfaceAudience not found - continuing with a stub.\n\nwarning: Class org.apache.hadoop.hbase.classification.InterfaceStability not found - continuing with a stub.\n\nwarning: Class org.apache.hadoop.hbase.classification.InterfaceAudience not found - continuing with a stub.\n\nwarning: Class org.apache.hadoop.hbase.classification.InterfaceAudience not found - continuing with a stub.\n\nwarning: Class org.apache.hadoop.hbase.classification.InterfaceAudience not found - continuing with a stub.\n\nwarning: Class org.apache.hadoop.hbase.classification.InterfaceStability not found - continuing with a stub.\n\nwarning: Class org.apache.hadoop.hbase.classification.InterfaceAudience not found - continuing with a stub.\n\nwarning: Class org.apache.hadoop.hbase.classification.InterfaceAudience not found - continuing with a stub.\n\ntdescriptor: org.apache.hadoop.hbase.HTableDescriptor = 'osm_nodes'\n\nwarning: Class org.apache.hadoop.hbase.classification.InterfaceAudience not found - continuing with a stub.\n\nwarning: Class org.apache.hadoop.hbase.classification.InterfaceStability not found - continuing with a stub.\n\nwarning: Class org.apache.hadoop.hbase.classification.InterfaceAudience not found - continuing with a stub.\n\nwarning: Class org.apache.hadoop.hbase.classification.InterfaceAudience not found - continuing with a stub.\n\nwarning: Class org.apache.hadoop.hbase.classification.InterfaceAudience not found - continuing with a stub.\n\nwarning: Class org.apache.hadoop.hbase.classification.InterfaceStability not found - continuing with a stub.\n\nres27: org.apache.hadoop.hbase.HTableDescriptor = 'osm_nodes', {NAME => 'node_meta', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n\nres28: org.apache.hadoop.hbase.HTableDescriptor = 'osm_nodes', {NAME => 'node_meta', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}, {NAME => 'node_tags', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n"}]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510164577536_-1730672340","id":"20171108-180937_1411840027","dateCreated":"2017-11-08T18:09:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3672","text":"iom.filter(\"type == 'node'\").foreachPartition({ partIter =>\n  val hConf = org.apache.hadoop.hbase.HBaseConfiguration.create()\n  hConf.addResource(new Path(\"/etc/hbase/conf/hbase-site.xml\"))\n  hConf.addResource(new Path(\"/etc/hadoop/conf/hadoop-site.xml\"))\n  val conn = ConnectionFactory.createConnection(hConf)\n  val table = conn.getTable(TableName.valueOf(NODE_TABLE))\n\n  partIter.foreach({ node =>\n    val id = node.getLong(0)\n    val tags = node.getAs[Map[String, String]](2)\n    val lat = Option(node.getDecimal(3)).map(_.doubleValue).getOrElse(Double.NaN)\n    val lon = Option(node.getDecimal(4)).map(_.doubleValue).getOrElse(Double.NaN)\n    val changeset = node.getLong(7)\n    val timestamp = node.getTimestamp(8).toInstant.toEpochMilli\n    val uid = node.getLong(9)\n    val user = node.getString(10)\n    val version = node.getLong(11)\n    val visible = node.getBoolean(12)\n    \n    val put = new Put(Bytes.toBytes(id) ++ Bytes.toBytes(timestamp / 3600000))\n    put.addColumn(NODE_META_CF, COLUMN_FID, Bytes.toBytes(id))\n    put.addColumn(NODE_META_CF, COLUMN_LAT, Bytes.toBytes(lat))\n    put.addColumn(NODE_META_CF, COLUMN_LON, Bytes.toBytes(lon))\n    put.addColumn(NODE_META_CF, COLUMN_USER, Bytes.toBytes(user))\n    put.addColumn(NODE_META_CF, COLUMN_VERSION, Bytes.toBytes(version))\n    put.addColumn(NODE_META_CF, COLUMN_VISIBLE, Bytes.toBytes(visible))\n    \n    tags.foreach { case (k, v) =>\n      put.addColumn(NODE_TAG_CF, Bytes.toBytes(k), Bytes.toBytes(v))\n    }\n    \n    table.put(put)\n  })\n})","dateUpdated":"2017-11-08T21:56:27+0000","dateFinished":"2017-11-08T21:41:42+0000","dateStarted":"2017-11-08T21:27:34+0000","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510177817570_-742057390","id":"20171108-215017_1855868769","dateCreated":"2017-11-08T21:50:17+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4759","text":"val wayTdescriptor = new HTableDescriptor(TableName.valueOf(WAY_TABLE))\nwayTdescriptor.addFamily(new HColumnDescriptor(WAY_META_CF))\nwayTdescriptor.addFamily(new HColumnDescriptor(WAY_TAG_CF))\nif (! hbAdmin.tableExists(TableName.valueOf(WAY_TABLE))) hbAdmin.createTable(wayTdescriptor)","dateUpdated":"2017-11-08T21:59:22+0000","dateFinished":"2017-11-08T21:59:28+0000","dateStarted":"2017-11-08T21:59:22+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nwayTdescriptor: org.apache.hadoop.hbase.HTableDescriptor = 'osm_ways'\n\nres49: org.apache.hadoop.hbase.HTableDescriptor = 'osm_ways', {NAME => 'way_meta', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n\nres50: org.apache.hadoop.hbase.HTableDescriptor = 'osm_ways', {NAME => 'way_meta', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}, {NAME => 'way_tags', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n"}]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510174488326_-1493817870","id":"20171108-205448_1897388399","dateCreated":"2017-11-08T20:54:48+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4352","text":"iom.filter(\"type == 'way'\").foreachPartition({ partIter =>\n  val hConf = org.apache.hadoop.hbase.HBaseConfiguration.create()\n  hConf.addResource(new Path(\"/etc/hbase/conf/hbase-site.xml\"))\n  hConf.addResource(new Path(\"/etc/hadoop/conf/hadoop-site.xml\"))\n  val conn = ConnectionFactory.createConnection(hConf)\n  val table = conn.getTable(TableName.valueOf(WAY_TABLE))\n\n  partIter.foreach({ way =>\n    val id = way.getLong(0)\n    val tags = way.getAs[scala.collection.immutable.Map[String, String]](2)\n    val nodes = {\n      // TODO: figure out why is this line needed to be so tortured and awful\n      val ns: Array[Long] = way.getAs[Seq[Row]](5).map({ case Row(l: Long) => l }).toArray\n      val bbuff = ByteBuffer.allocate(ns.size * 8)\n      val lbuff = bbuff.asLongBuffer\n      lbuff.put(ns)\n      bbuff.array\n    }\n    val changeset = way.getLong(7)\n    val timestamp = way.getTimestamp(8).toInstant.toEpochMilli\n    val uid = way.getLong(9)\n    val user = way.getString(10)\n    val version = way.getLong(11)\n    val visible = way.getBoolean(12)\n    \n    val put = new Put(Bytes.toBytes(id) ++ Bytes.toBytes(timestamp / 3600000))\n    put.addColumn(WAY_META_CF, COLUMN_FID, Bytes.toBytes(id))\n    put.addColumn(WAY_META_CF, COLUMN_NODES, nodes)\n    put.addColumn(WAY_META_CF, COLUMN_USER, Bytes.toBytes(user))\n    put.addColumn(WAY_META_CF, COLUMN_VERSION, Bytes.toBytes(version))\n    put.addColumn(WAY_META_CF, COLUMN_VISIBLE, Bytes.toBytes(visible))\n    \n    tags.foreach { case (k, v) =>\n      put.addColumn(WAY_TAG_CF, Bytes.toBytes(k), Bytes.toBytes(v))\n    }\n    \n    table.put(put)\n  })\n})","dateUpdated":"2017-11-08T22:44:46+0000","dateFinished":"2017-11-08T22:40:50+0000","dateStarted":"2017-11-08T22:39:33+0000","results":{"code":"SUCCESS","msg":[]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510180967587_2004941890","id":"20171108-224247_1125917606","dateCreated":"2017-11-08T22:42:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5712","text":"val REL_TABLE = Bytes.toBytes(\"osm_relations\")\nval REL_META_CF = Bytes.toBytes(\"rel_meta\")\nval REL_TAG_CF = Bytes.toBytes(\"rel_tags\")","dateUpdated":"2017-11-08T22:43:13+0000","dateFinished":"2017-11-08T22:43:16+0000","dateStarted":"2017-11-08T22:43:13+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nREL_TABLE: Array[Byte] = Array(111, 115, 109, 95, 114, 101, 108, 97, 116, 105, 111, 110, 115)\n\nREL_META_CF: Array[Byte] = Array(114, 101, 108, 95, 109, 101, 116, 97)\n\nREL_TAG_CF: Array[Byte] = Array(114, 101, 108, 95, 116, 97, 103, 115)\n"}]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510178372593_372545373","id":"20171108-215932_1599749847","dateCreated":"2017-11-08T21:59:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4974","text":"val relTdescriptor = new HTableDescriptor(TableName.valueOf(REL_TABLE))\nrelTdescriptor.addFamily(new HColumnDescriptor(REL_META_CF))\nrelTdescriptor.addFamily(new HColumnDescriptor(REL_TAG_CF))\nif (! hbAdmin.tableExists(TableName.valueOf(REL_TABLE))) hbAdmin.createTable(relTdescriptor)","dateUpdated":"2017-11-08T22:43:42+0000","dateFinished":"2017-11-08T22:43:51+0000","dateStarted":"2017-11-08T22:43:42+0000","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nrelTdescriptor: org.apache.hadoop.hbase.HTableDescriptor = 'osm_relations'\n\nres82: org.apache.hadoop.hbase.HTableDescriptor = 'osm_relations', {NAME => 'rel_meta', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n\nres83: org.apache.hadoop.hbase.HTableDescriptor = 'osm_relations', {NAME => 'rel_meta', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}, {NAME => 'rel_tags', BLOOMFILTER => 'ROW', VERSIONS => '1', IN_MEMORY => 'false', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', TTL => 'FOREVER', COMPRESSION => 'NONE', MIN_VERSIONS => '0', BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}\n"}]}},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1510180999963_1434221354","id":"20171108-224319_578626971","dateCreated":"2017-11-08T22:43:19+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5793","text":"iom.filter(\"type == 'relation'\").foreachPartition({ partIter =>\n  val hConf = org.apache.hadoop.hbase.HBaseConfiguration.create()\n  hConf.addResource(new Path(\"/etc/hbase/conf/hbase-site.xml\"))\n  hConf.addResource(new Path(\"/etc/hadoop/conf/hadoop-site.xml\"))\n  val conn = ConnectionFactory.createConnection(hConf)\n  val table = conn.getTable(TableName.valueOf(WAY_TABLE))\n\n  partIter.foreach({ rel =>\n    val id = rel.getLong(0)\n    val tags = rel.getAs[scala.collection.immutable.Map[String, String]](2)\n    val members = ??? // TODO: figure out how to actually go about storing this list of tuples\n    val changeset = rel.getLong(7)\n    val timestamp = rel.getTimestamp(8).toInstant.toEpochMilli\n    val uid = rel.getLong(9)\n    val user = rel.getString(10)\n    val version = rel.getLong(11)\n    val visible = rel.getBoolean(12)\n    \n    val put = new Put(Bytes.toBytes(id) ++ Bytes.toBytes(timestamp / 3600000))\n    put.addColumn(REL_META_CF, COLUMN_FID, Bytes.toBytes(id))\n    put.addColumn(REL_META_CF, COLUMN_NODES, nodes)\n    put.addColumn(REL_META_CF, COLUMN_USER, Bytes.toBytes(user))\n    put.addColumn(REL_META_CF, COLUMN_VERSION, Bytes.toBytes(version))\n    put.addColumn(REL_META_CF, COLUMN_VISIBLE, Bytes.toBytes(visible))\n    \n    tags.foreach { case (k, v) =>\n      put.addColumn(REL_TAG_CF, Bytes.toBytes(k), Bytes.toBytes(v))\n    }\n\n    table.put(put)\n  })\n})","dateUpdated":"2017-11-08T22:48:38+0000"}],"name":"test","id":"2CXPXDDBM","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}