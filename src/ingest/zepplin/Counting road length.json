{"paragraphs":[{"text":"import geotrellis.proj4._\nimport geotrellis.spark._\nimport geotrellis.spark.io._\nimport geotrellis.spark.io.file._\nimport geotrellis.spark.io.index.ZCurveKeyIndexMethod\nimport geotrellis.spark.io.s3.{S3AttributeStore, S3LayerWriter}\nimport geotrellis.spark.tiling._\nimport geotrellis.vector._\nimport geotrellis.vectortile.VectorTile\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql._\nimport vectorpipe._\nimport vectorpipe.util.LayerMetadata\nimport cats.implicits._\nimport com.monovore.decline._\n","dateUpdated":"2017-09-18T01:43:35+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport geotrellis.proj4._\n\nimport geotrellis.spark._\n\nimport geotrellis.spark.io._\n\nimport geotrellis.spark.io.file._\n\nimport geotrellis.spark.io.index.ZCurveKeyIndexMethod\n\nimport geotrellis.spark.io.s3.{S3AttributeStore, S3LayerWriter}\n\nimport geotrellis.spark.tiling._\n\nimport geotrellis.vector._\n\nimport geotrellis.vectortile.VectorTile\n\nimport org.apache.log4j.{Level, Logger}\n\nimport org.apache.spark._\n\nimport org.apache.spark.rdd.RDD\n\nimport org.apache.spark.sql._\n\nimport vectorpipe._\n\nimport vectorpipe.util.LayerMetadata\n\nimport cats.implicits._\n\nimport com.monovore.decline._\n"}]},"apps":[],"jobName":"paragraph_1505692571638_-415547723","id":"20170912-192002_1003937889","dateCreated":"2017-09-17T23:56:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7043","user":"anonymous","dateFinished":"2017-09-18T01:44:10+0000","dateStarted":"2017-09-18T01:43:35+0000"},{"text":"      /* Settings compatible for both local and EMR execution */\n      val conf = new SparkConf()\n        .setIfMissing(\"spark.master\", \"local[*]\")\n        .setAppName(\"vp-orc-io\")\n\n      implicit val ss: SparkSession = SparkSession.builder\n        .config(conf)\n        .enableHiveSupport\n        .getOrCreate\n","dateUpdated":"2017-09-18T01:44:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nconf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@5ecc88b0\n\nss: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@54e711a5\n"}]},"apps":[],"jobName":"paragraph_1505692571638_-415547723","id":"20170912-192013_1465160975","dateCreated":"2017-09-17T23:56:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7044","user":"anonymous","dateFinished":"2017-09-18T01:44:53+0000","dateStarted":"2017-09-18T01:44:51+0000"},{"text":"val orc = \"s3://osm-pds/planet/planet-latest.orc\"\n//val orc = \"s3://vectortiles/orc/europe/luxembourg.orc\"\n//val orc = \"s3://vectortiles/orc/north-america/mexico.orc\"\n//val df = ss.read.orc(\"s3://vectortiles/orc/europe/finland.orc\")\nval df = ss.read.orc(orc)\n//val df = ss.read.orc(\"s3://osm-pds/planet/planet-latest.orc\")","dateUpdated":"2017-09-18T04:05:26+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\norc: String = s3://osm-pds/planet/planet-latest.orc\n\ndf: org.apache.spark.sql.DataFrame = [id: bigint, type: string ... 10 more fields]\n"}]},"apps":[],"jobName":"paragraph_1505692571639_-415932472","id":"20170912-195450_1591677940","dateCreated":"2017-09-17T23:56:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7045","user":"anonymous","dateFinished":"2017-09-18T04:05:28+0000","dateStarted":"2017-09-18T04:05:26+0000"},{"text":"def countRoads(rdd: RDD[osm.OSMFeature]): Double = {\n    val EARTH_RADIUS = 6378137 // Use what gdal2tiles uses.    \n    val EARTH_CIRCUMFERENCE = 2 * math.Pi * EARTH_RADIUS\n    val p = math.Pi / 180       \n    \n    val tags =\n        Set(\n            \"motorway\",\n\"trunk\",\n\"motorway_link\",\n\"trunk_link\",\n\"primary\",\n\"secondary\",\n\"tertiary\",\n\"primary_link\",\n\"secondary_link\",\n\"tertiary_link\",\n\"service\",\n\"residential\",\n\"unclassified\",\n\"living_street\",\n\"road\"\n            )\n\n    rdd\n        .flatMap { f => \n            f.geom match { \n                case l: Line =>\n                    if(f.data.root.tagMap.contains(\"highway\")) {\n                        f.data.root.tagMap.get(\"highway\") match {\n                             case Some(t) if tags.contains(t) => \n                               Some(l) \n                             case _ => None\n                         }\n                    }   \n                    else { None }\n                case _ => None\n            }\n        }\n        .map { l: Line =>\n            val coords = l.vertices\n            val len = coords.length\n            var i = 0\n            var dist = 0.0\n            while(i < len - 1) {\n                // calculate the length of the road with Haversine distance formula\n                val ll1 = coords(i)\n                val ll2 = coords(i + 1)\n            //    val a = 0.5 - math.cos((ll2.y - ll1.y) * p) / 2 + math.cos(ll1.y * p) * math.cos(ll2.y * p) * (1 - math.cos((ll2.x - ll1.x) * p)) / 2\n//                dist += (2 * EARTH_CIRCUMFERENCE * math.asin(math.sqrt(a)))\n\n\n    val R = EARTH_RADIUS\n    val (start, end) = (ll1, ll2)\n    val dLat = math.toRadians(end.y - start.y)\n    val dLon = math.toRadians(end.x - start.x)\n    val lat1 = math.toRadians(start.y)\n    val lat2 = math.toRadians(end.y)\n\n    val a = math.sin(dLat/2) * math.sin(dLat/2) +\n            math.sin(dLon/2) * math.sin(dLon/2) * math.cos(lat1) * math.cos(lat2)\n    val c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n\n    dist += R * c\n\n\n                i += 1\n            }\n            dist / 1000.0 // KM\n        }\n        .reduce(_ + _)\n}","dateUpdated":"2017-09-18T03:59:32+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ncountRoads: (rdd: org.apache.spark.rdd.RDD[vectorpipe.osm.OSMFeature])Double\n"}]},"apps":[],"jobName":"paragraph_1505692571639_-415932472","id":"20170912-195924_708474401","dateCreated":"2017-09-17T23:56:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7046","user":"anonymous","dateFinished":"2017-09-18T03:56:42+0000","dateStarted":"2017-09-18T03:56:41+0000"},{"text":"      val layout: LayoutDefinition =\n        ZoomedLayoutScheme.layoutForZoom(14, WebMercator.worldExtent, 512)\n\n\n      osm.fromORC(orc) match {\n        case Left(e) => println(e)\n        case Right((ns,ws,rs)) => {\n          val nodePartitioner = new HashPartitioner(1000)\n          val wayPartitioner = new HashPartitioner(1000)\n\n          /* Assumes that OSM ORC is in LatLng */\n//          val feats: RDD[osm.OSMFeature] =\n//            osm.toFeatures(ns.partitionBy(nodePartitioner), ws.partitionBy(wayPartitioner), rs)\n          val (pnt, lns, pls, mps) =\n            osm.toFeatures2(ns.partitionBy(nodePartitioner), ws.partitionBy(wayPartitioner), rs)\n            \n         //println(pnt.count())\n         //println(lns.count())\n         //println(pls.count())\n         //println(mps.count())\n         \n         val feats: RDD[osm.OSMFeature] =  ns.sparkContext.union(pnt, lns, pls, mps)\n            \n         val km =  countRoads(lns)\n         println(s\"TOTAL DISTANCE: ${km.toLong} KM\")\n            \n//          feats.count()\n\n          /* Associated each Feature with a SpatialKey */\n//          val fgrid: RDD[(SpatialKey, Iterable[osm.OSMFeature])] =\n//            VectorPipe.toGrid(Clip.byHybrid, VectorPipe.log4j, layout, feats)\n\n          /* Create the VectorTiles */\n//          val tiles: RDD[(SpatialKey, VectorTile)] =\n//            VectorPipe.toVectorTile(Collate.withoutMetadata, layout, fgrid)\n            \n//          tiles.count()\n        }\n      }\n","dateUpdated":"2017-09-18T04:05:58+0000","config":{"colWidth":11,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"\nlayout: geotrellis.spark.tiling.LayoutDefinition = GridExtent(Extent(-2.0037508342789244E7, -2.0037508342789244E7, 2.0037508342789244E7, 2.0037508342789244E7),4.777314267823516,4.777314267823516)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 102 in stage 143.0 failed 4 times, most recent failure: Lost task 102.3 in stage 143.0 (TID 14483, ip-172-31-22-132.ec2.internal, executor 706): ExecutorLostFailure (executor 706 exited caused by one of the running tasks) Reason: Container killed by YARN for exceeding memory limits. 5.5 GB of 5.5 GB physical memory used. Consider boosting spark.yarn.executor.memoryOverhead.\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1569)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1557)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1556)\n  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1556)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:815)\n  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:815)\n  at scala.Option.foreach(Option.scala:257)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:815)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1784)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1739)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1728)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:631)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119)\n  at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008)\n  at $$$$68fcb2421412a821f22fc798f81019bf$$$countRoads(<console>:138)\n  ... 68 elided\n"}]},"apps":[],"jobName":"paragraph_1505692571639_-415932472","id":"20170912-200623_759453934","dateCreated":"2017-09-17T23:56:11+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:7047","user":"anonymous","dateFinished":"2017-09-18T04:48:25+0000","dateStarted":"2017-09-18T04:05:58+0000"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1505707255575_-1758441065","id":"20170918-040055_139899382","dateCreated":"2017-09-18T04:00:55+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9714"}],"name":"Counting road length","id":"2CTTZVW67","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}